# UNIT 01

## 1. 강화학습이란
- 강화학습 : agent가 환경과 상호작용하고 작업 수행에 대한 보상을 통해 학습 진행

## 2. 강화학습의 구성
- 프레임워크
    - State : agent가 환경으로부터 얻는 정보 / 세계 상태에 대한 완전한 정보, 관찰 (숨겨진 정보 없음)
        - Observation : State에 대한 부분 설명 (부분적으로 관찰된 환경)
    - Action : 무한 / 유한한 행동
    - Reward : agent에 대한 유일한 피드백
        - 각 시간 단계 t에 대한 보상 : $R(\tau) = r_{t+1}+r_{t+2}+r_{t+3}+...$
        - $r$ : 할인율 - 값이 작을수록 agent가 단기 보상에 더 큰 관심 가지는 것
    - Next State
- 순서
    - 환경으로부터 상태($S_0$)를 받게 됨
    - 상태($S_0$)를 기반으로 행동($A_0$)을 취함
    - 환경은 새로운 상태($S_1$)로 전환됨
    - 보상($R_1$) 획득
- Agent의 목표 : 기대 수익 (누적 보상) 최대화
    - 누적 보상을 최대화하는 행동 취하는 것을 목표로 움직임
- Markov Property : 수행 작업 결정에 있어 현재 상태만을 기반으로 결정 (과거 모든 상태, 작업의 기록 활용하지 않음)

## 3. Task의 종류
- Episodic : 시작과 끝이 존재하는 작업
- Continuing : 시작만이 존재하는 작업, agent가 중지하기로 결정할 때까지 계속 실행

## 4. Exploration/Exploitation Tradeoff
- Exploration (탐색) : 환경에 대한 더 많은 정보를 찾기 위한 행위
- Exploitation (활용) : 보상을 극대화하기 위해 알려진 정보를 활용하는 행위
- 환경을 탐험하는 정도와 환경에 대해 알고 있는 것을 활용하는 정도의 균형을 맞춰야 함

## 5. 강화학습을 위한 두가지 주요 접근 방식
- 정책 기반
    - 정책 : 주어진 상태에서 어떠한 조치를 취해야 하는지 알려주는 법칙 / 특정 시점의 agent의 행동을 정의함
    - 종류
        - 결정적 : 주어진 상태의 정책은 항상 동일한 행위를 반환
        - 확률적 : 행동에 대한 확률분포 출력
- 가치 기반 : 해당 상태의 예상값에 매핑하는 가치 함수 학습
    - 상태의 가치 : 해당 상태에서 시작하여 정책에 따라 행동할 경우 얻을 수 있는 누적 보상
    - 에이전트가 각 상태에서 얻게 될 예상 수익을 알려주는 가치 함수를 교육하고 이 함수를 사용하여 정책을 정의

## 6. 심층 강화학습
- 강화학습 해결을 위해 심층 신경망 활용
- 예시
    - Q-learning : 전통적인 알고리즘 활용하여 각 state에 대해 수행할 action을 찾는 데에 도움이 되는 Q 테이블 제작
    - Deep Q-learning : 신경망을 활용하여 Q 값을 근사

# 퀴즈
1. 강화학습이란 무엇입니까?
- agent가 환경과 상호작용하고 그 결과로 보상을 얻으며 이를 기반으로 학습해 나아가는 방식
2. RL loop
- agent가 환경으로부터 state 얻음
- state에 기반하여 agent가 action 취함
- action 취함에 따라 새로운 state로 이동됨
- 새로운 state에서 reward 획득
3. 상태와 관찰의 차이점은 무엇입니까?
- 상태 : 세계에 대한 완전한 설명
- 관찰 : 세계에 대한 일부 설명
4. state의 두가지 종류는 무엇입니까?
- episodic
- continuing
5. 정책이란 무엇입니까?
- 특정 상태에서 어떠한 행위를 취해야할 지 알려주는 기준
6. 가치 기반 방법이란 무엇입니까?
- 각 state의 예상 reward에 대해 매핑하는 가치 함수 학습 방식
7. 정책 기반 방법이란 무엇입니까?
- 각 state에 가장 적합한 action에 매핑하는 방식, 가능한 action 집합에 대한 확률 분포 ㄴ