# RLHF
## 배경 설명
- 언어모델의 손실 함수 : 크로스 엔트로피
- 언어모델의 평가 지표 : BLEU, ROUGE 
    - 인간의 선호도를 더 잘 포착하도록 설계된 메트릭
    - 생성된 텍스트를 간단한 규칙이 있는 reference와 단순히 비교하게 때문에 제한적이라는 한계 존재
- 생성된 텍스트에 대한 사람의 피드백을 성능 측정으로 활용하거나 한단계 더 나아가 해당 피드백을 손실함수르 활용하여 모델을 최적화하는 것은 어떨까?

## 단계
- 언어모델 사전 학습
- 데이터 수집 및 보상 모델 학습
    - 인간의 선호도로 보정된 보상 모델 (RM)
    - 일련의 텍스트를 받아들여 인간의 선호도를 수치적으로 나타내는 scalar 보상을 반환하는 모델/시스템
    - 인간 어노테이터가 LM에서 생성한 텍스트 출력의 순위를 매김
        - 순위 매김 방식 예시 : 동일 프롬프트에서 조건이 지정된 두 언어 모델에서 생성한 텍스트 비교
- 강화학습을 통해 언어모델 미세조정
    - Proximal Policy Optimization 활용하여 초기 LM의 일부 혹은 모든 매개변수 미세조정
    - reward의 종류
        - $r_{\theta}$ : 원래의 프롬프트와 연결된 텍스트는 'preferability'라는 scalar 개념을 반환하는 선호 모델로 전달됨
        - $r_{KL}$ : 토큰별 확률 분포를 초기 모델의 확률 분포와 비교하여 차이에 대한 penalty 계산, 초기의 사전학습된 모델에서 크게 벗어나지 않도록 조정
    - 최종 reward : $r = r_{\theta} + r_{KL}$
- 업데이트 규칙
    - 현재 데이터 batch에서 reward 지표를 최대화하는 PPO의 매개변수 업데이트
    - 학습 프로세스를 불안정하게 만들지 않기 위해 gradient에 대한 제약조건을 활용하는 trust region optimization 방식

## 추가 자료
[Transformer with RL](https://github.com/huggingface/trl)
